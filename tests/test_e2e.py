"""E2E í…ŒìŠ¤íŠ¸

ì‹¤ì œ ONNX ëª¨ë¸ì„ ì‚¬ìš©í•œ ì—”ë“œíˆ¬ì—”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ìŠ¤í‚µë©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
    1. ëª¨ë¸ ì¤€ë¹„: python scripts/download_model.py
    2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: pytest tests/test_e2e.py -v -s
"""

from pathlib import Path

import pytest

from airgap_kor_search.config import (
    AppConfig,
    ChunkConfig,
    IndexConfig,
    ModelConfig,
    SearchConfig,
)
from airgap_kor_search.searcher import SearchEngine


# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MODEL_DIR = Path("./airgap_data/model")
FIXTURES_DIR = Path(__file__).parent / "fixtures"

HAS_MODEL = (MODEL_DIR / "model.onnx").exists() and (
    MODEL_DIR / "tokenizer.json"
).exists()

skip_no_model = pytest.mark.skipif(
    not HAS_MODEL,
    reason=f"ONNX ëª¨ë¸ ì—†ìŒ ({MODEL_DIR}). "
    "python scripts/download_model.py ë¡œ ëª¨ë¸ì„ ì¤€ë¹„í•˜ì„¸ìš”.",
)


# â”€â”€ í”½ìŠ¤ì²˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@pytest.fixture(scope="module")
def engine(tmp_path_factory):
    """ëª¨ë“ˆ ë ˆë²¨ SearchEngine (ëª¨ë¸ ë¡œë“œê°€ ëŠë¦¬ë¯€ë¡œ í•œ ë²ˆë§Œ)"""
    tmp_path = tmp_path_factory.mktemp("e2e")

    config = AppConfig(
        data_dir=tmp_path / "data",
        model=ModelConfig(
            model_dir=MODEL_DIR,
            embedding_dim=1024,
            max_seq_length=512,
            batch_size=8,
        ),
        chunk=ChunkConfig(chunk_size=256, chunk_overlap=32, min_chunk_length=30),
        index=IndexConfig(
            index_path=tmp_path / "data" / "index.faiss",
            db_path=tmp_path / "data" / "meta.db",
        ),
        search=SearchConfig(top_k=5),
    )

    engine = SearchEngine.from_config(config)
    engine.open()

    # ìƒ˜í”Œ ë¬¸ì„œ ì¸ë±ì‹±
    if FIXTURES_DIR.exists():
        result = engine.index_directory(FIXTURES_DIR)
        print(
            f"\nğŸ“š E2E ì¸ë±ì‹±: {result.documents_processed}ê°œ ë¬¸ì„œ, "
            f"{result.chunks_created}ê°œ ì²­í¬ ({result.elapsed_sec:.1f}ì´ˆ)"
        )

    yield engine
    engine.close()


# â”€â”€ E2E í…ŒìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@skip_no_model
class TestE2ESearch:
    """ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""

    def test_basic_search(self, engine):
        """ê¸°ë³¸ ê²€ìƒ‰ì´ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€"""
        response = engine.search("í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„")

        assert response.total_found > 0
        print(f"\nğŸ” 'í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„' â†’ {response.total_found}ê±´")
        for r in response.results:
            print(f"   [{r.score_percent}%] {r.text[:60]}...")

    def test_semantic_search(self, engine):
        """ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ê²°ê³¼ê°€ ìƒìœ„ì— ì˜¤ëŠ”ì§€"""
        response = engine.search("ì˜¤í”„ë¼ì¸ì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” ê²€ìƒ‰")

        assert response.total_found > 0
        # ì—ì–´ê°­/ì˜¤í”„ë¼ì¸/ê²€ìƒ‰ ê´€ë ¨ ë‚´ìš©ì´ ìƒìœ„ì— ì™€ì•¼ í•¨
        top_text = response.results[0].text
        has_relevant = any(
            keyword in top_text
            for keyword in ["ì—ì–´ê°­", "ì˜¤í”„ë¼ì¸", "ì¸í„°ë„·", "ì°¨ë‹¨", "ë²¡í„°",
                            "ê²€ìƒ‰", "FAISS", "SQLite"]
        )
        print(f"\nğŸ” 'ì˜¤í”„ë¼ì¸ì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” ê²€ìƒ‰' â†’ ìƒìœ„ ê²°ê³¼:")
        print(f"   [{response.results[0].score_percent}%] {top_text[:80]}...")
        assert has_relevant, f"ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ê°€ 1ìœ„: {top_text[:80]}"

    def test_korean_synonym_search(self, engine):
        """í•œêµ­ì–´ ë™ì˜ì–´/ìœ ì‚¬ í‘œí˜„ ê²€ìƒ‰"""
        response = engine.search("ë¬¸ì¥ì„ ìˆ«ì ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” ê¸°ìˆ ")

        assert response.total_found > 0
        # ì„ë² ë”© ê´€ë ¨ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•¨
        all_text = " ".join(r.text for r in response.results)
        has_embedding = any(
            keyword in all_text
            for keyword in ["ì„ë² ë”©", "ë²¡í„°", "ë³€í™˜", "BERT"]
        )
        print(f"\nğŸ” 'ë¬¸ì¥ì„ ìˆ«ì ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” ê¸°ìˆ ' â†’ {response.total_found}ê±´")
        assert has_embedding

    def test_search_relevance_order(self, engine):
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœì¸ì§€"""
        response = engine.search("FAISS ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬")

        if len(response.results) >= 2:
            scores = [r.score for r in response.results]
            assert scores == sorted(scores, reverse=True), "ê²°ê³¼ê°€ ìœ ì‚¬ë„ ìˆœì´ ì•„ë‹™ë‹ˆë‹¤"

        print(f"\nğŸ” ì ìˆ˜ ìˆœì„œ: {[r.score_percent for r in response.results]}")

    def test_different_queries_different_results(self, engine):
        """ë‹¤ë¥¸ ì¿¼ë¦¬ê°€ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€"""
        r1 = engine.search("í˜•íƒœì†Œ ë¶„ì„ê¸° ì¢…ë¥˜")
        r2 = engine.search("ì—ì–´ê°­ ë³´ì•ˆ í™˜ê²½")

        if r1.total_found > 0 and r2.total_found > 0:
            top1 = r1.results[0].text[:50]
            top2 = r2.results[0].text[:50]
            # ì™„ì „íˆ ê°™ì€ ê²°ê³¼ê°€ ì•„ë‹ˆì–´ì•¼ í•¨
            assert top1 != top2, "ì„œë¡œ ë‹¤ë¥¸ ì¿¼ë¦¬ì¸ë° ê°™ì€ ê²°ê³¼"

        print(f"\nğŸ” ì¿¼ë¦¬1 ìƒìœ„: {r1.results[0].text[:50]}...")
        print(f"ğŸ” ì¿¼ë¦¬2 ìƒìœ„: {r2.results[0].text[:50]}...")


@skip_no_model
class TestE2EIndexing:
    """ì¸ë±ì‹± ê´€ë ¨ E2E í…ŒìŠ¤íŠ¸"""

    def test_stats(self, engine):
        """ì¸ë±ìŠ¤ í†µê³„ê°€ ì •ìƒì¸ì§€"""
        stats = engine.get_stats()

        assert stats["total_documents"] >= 3
        assert stats["total_chunks"] >= 3
        assert stats["total_vectors"] == stats["total_chunks"]

        print(f"\nğŸ“Š í†µê³„: {stats['total_documents']}ë¬¸ì„œ, "
              f"{stats['total_chunks']}ì²­í¬, {stats['total_vectors']}ë²¡í„°")

    def test_list_documents(self, engine):
        """ë¬¸ì„œ ëª©ë¡ì´ ì •ìƒì¸ì§€"""
        docs = engine.list_documents()

        assert len(docs) >= 3
        for doc in docs:
            assert doc["chunk_count"] > 0

        print(f"\nğŸ“‹ ë¬¸ì„œ ëª©ë¡:")
        for doc in docs:
            print(f"   {doc['doc_path']} ({doc['chunk_count']}ì²­í¬)")

    def test_index_text_directly(self, engine):
        """í…ìŠ¤íŠ¸ ì§ì ‘ ì¸ë±ì‹± í›„ ê²€ìƒ‰"""
        engine.index_text(
            "íŒŒì´ì¬ì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì„ ê°€ì§„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. "
            "ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ, ì¸ê³µì§€ëŠ¥ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. "
            "í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœê³„ê°€ íŒŒì´ì¬ì˜ ê°€ì¥ í° ì¥ì ì…ë‹ˆë‹¤.",
            source="e2e_direct_input",
        )

        response = engine.search("íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì¥ì ")
        assert response.total_found > 0

        # ì§ì ‘ ì…ë ¥í•œ í…ìŠ¤íŠ¸ê°€ ê²°ê³¼ì— í¬í•¨ë˜ì–´ì•¼ í•¨
        sources = [r.doc_path for r in response.results]
        assert "e2e_direct_input" in sources

        print(f"\nğŸ” ì§ì ‘ ì…ë ¥ ê²€ìƒ‰: {response.results[0].score_percent}%")

        # ì •ë¦¬
        engine.delete_document("e2e_direct_input")