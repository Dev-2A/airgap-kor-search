"""ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & ONNX ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸

ì¸í„°ë„·ì´ ë˜ëŠ” í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ íŒŒì¼ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    pip install optimum[onnxruntime] transformers torch
    python scripts/download_model.py
    python scripts/download_model.py --model intfloat/multilingual-e5-small --dim 384
"""

from __future__ import annotations

import argparse
import shutil
import sys
from pathlib import Path


SUPPORTED_MODELS = {
    "BAAI/bge-m3": {"dim": 1024, "pooling": "cls"},
    "intfloat/multilingual-e5-large-instruct": {"dim": 1024, "pooling": "mean"},
    "intfloat/multilingual-e5-small": {"dim": 384, "pooling": "mean"},
}


def download_and_convert(
    model_name: str,
    output_dir: Path,
    quantize: bool = False,
) -> None:
    """ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ONNXë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        from optimum.onnxruntime import ORTModelForFeatureExtraction
    except ImportError:
        print("âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("   pip install optimum[onnxruntime] transformers torch")
        sys.exit(1)

    print(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}")
    print(f"   (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print()

    # ONNX ë³€í™˜ + ë‹¤ìš´ë¡œë“œ
    tmp_dir = output_dir / "_tmp_onnx"
    model = ORTModelForFeatureExtraction.from_pretrained(
        model_name, export=True
    )
    model.save_pretrained(str(tmp_dir))

    # tokenizerë„ ì €ì¥
    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(str(tmp_dir))

    # í•„ìš”í•œ íŒŒì¼ë§Œ ì¶”ì¶œ
    output_dir.mkdir(parents=True, exist_ok=True)

    # ONNX ëª¨ë¸ íŒŒì¼
    onnx_file = tmp_dir / "model.onnx"
    if not onnx_file.exists():
        # ì¼ë¶€ ëª¨ë¸ì€ ë‹¤ë¥¸ ì´ë¦„ì¼ ìˆ˜ ìˆìŒ
        onnx_files = list(tmp_dir.glob("*.onnx"))
        if onnx_files:
            onnx_file = onnx_files[0]
        else:
            print("âŒ ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)

    shutil.copy2(onnx_file, output_dir / "model.onnx")
    print(f"âœ… model.onnx ({_file_size(output_dir / 'model.onnx')})")

    # í† í¬ë‚˜ì´ì €
    tokenizer_file = tmp_dir / "tokenizer.json"
    if tokenizer_file.exists():
        shutil.copy2(tokenizer_file, output_dir / "tokenizer.json")
        print(f"âœ… tokenizer.json ({_file_size(output_dir / 'tokenizer.json')})")
    else:
        print("âš ï¸ tokenizer.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë³µì‚¬í•´ì£¼ì„¸ìš”.")

    # ì–‘ìí™” (ì„ íƒ)
    if quantize:
        print("\nâš¡ ì–‘ìí™” ì§„í–‰ ì¤‘...")
        try:
            from optimum.onnxruntime import ORTQuantizer
            from optimum.onnxruntime.configuration import AutoQuantizationConfig

            quantizer = ORTQuantizer.from_pretrained(str(tmp_dir))
            qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False)

            quant_dir = output_dir / "_quantized"
            quantizer.quantize(
                save_dir=str(quant_dir), quantization_config=qconfig
            )

            quant_onnx = quant_dir / "model_quantized.onnx"
            if quant_onnx.exists():
                shutil.copy2(quant_onnx, output_dir / "model_quantized.onnx")
                print(
                    f"âœ… model_quantized.onnx ({_file_size(output_dir / 'model_quantized.onnx')})"
                )

            shutil.rmtree(quant_dir, ignore_errors=True)
        except Exception as e:
            print(f"âš ï¸ ì–‘ìí™” ì‹¤íŒ¨: {e}")

    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    shutil.rmtree(tmp_dir, ignore_errors=True)

    # ëª¨ë¸ ì •ë³´ íŒŒì¼
    info = SUPPORTED_MODELS.get(model_name, {})
    info_text = (
        f"model: {model_name}\n"
        f"dim: {info.get('dim', 'unknown')}\n"
        f"pooling: {info.get('pooling', 'unknown')}\n"
    )
    (output_dir / "model_info.txt").write_text(info_text)

    print(f"\nğŸ‰ ì™„ë£Œ! ëª¨ë¸ íŒŒì¼ ìœ„ì¹˜: {output_dir}")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  ì—ì–´ê°­ í™˜ê²½ìœ¼ë¡œ {output_dir} ë””ë ‰í† ë¦¬ë¥¼ ë³µì‚¬í•˜ì„¸ìš”.")


def _file_size(path: Path) -> str:
    """íŒŒì¼ í¬ê¸°ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    size = path.stat().st_size
    for unit in ["B", "KB", "MB", "GB"]:
        if size < 1024:
            return f"{size:.1f}{unit}"
        size /= 1024
    return f"{size:.1f}TB"


def main():
    parser = argparse.ArgumentParser(
        description="ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ & ONNX ë³€í™˜"
    )
    parser.add_argument(
        "--model",
        default="BAAI/bge-m3",
        help=f"ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: BAAI/bge-m3). ì§€ì›: {', '.join(SUPPORTED_MODELS)}",
    )
    parser.add_argument(
        "--output",
        default="./airgap_data/model",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./airgap_data/model)",
    )
    parser.add_argument(
        "--quantize",
        action="store_true",
        help="ì–‘ìí™” ë²„ì „ë„ í•¨ê»˜ ìƒì„±",
    )

    args = parser.parse_args()
    download_and_convert(
        model_name=args.model,
        output_dir=Path(args.output),
        quantize=args.quantize,
    )


if __name__ == "__main__":
    main()